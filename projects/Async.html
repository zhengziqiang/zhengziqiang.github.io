<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>The Synthesis of Unpaired Underwater Images for Monocular Underwater Depth Prediction</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Asynchronous generative adversarial network for asymmetric unpaired image-to-image translation">
<meta name="keywords" content="Webpage analysis; Asynchronous learning; Image translation; Information asymmetry">
<meta name="keywords" content="Ziqiang Zheng; 郑自强; Computer Vision;">
<link rel="author" href="https://zhengziqiang.github.io/">

<!-- Fonts and stuff -->
<link href="./css/css" rel="styles<!-- heet" type="text/css">
<link rel="stylesheet" type="text/css" href="./css/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./OST/iconize.css">
<script async="" src="./css/prettify.js.download"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
	<h1>Asynchronous generative adversarial network for asymmetric unpaired image-to-image translation</h1>

	<div class="authors">
        <a href="https://zhengziqiang.github.io/">Ziqiang Zheng#</a>
	  <a href="https://scholar.google.com/citations?user=KDdkZKQAAAAJ&hl=zh-CN&oi=ao">Yi Bin#</a>
        <a href="https://www.facebook.com/lv.xiaoou">Xiaoou Lv</a>&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com.hk/citations?user=vwOQ-UIAAAAJ&hl=zh-CN/">Yang Wu</a>
	  <a href="https://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>
	  <a href="https://cfm.uestc.edu.cn/~shenht/">Heng Tao Shen</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</div>

	<div class="affiliations">

	  <p><a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a></p>
	</div class='section teaser'>

	<!-- <div class="venue">Preprint Manuscript (<a href="https://arxiv.org/" target="_blank">Arxiv</a>) 2018</div> -->
      </div>

      <center><img src="./Async-GAN/entropy.jpg" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
The unpaired image-to-image translation aims to translate input images from one source domain to some desired outputs in a target domain by learning from unpaired training data. Cycle-consistency constraint provides a general principle to estimate and measure forward and backward mapping functions between two domains. In many cases, the information entropy of images from the two domains is not equal, resulting in an information-rich domain and an information-poor domain. However, existing solutions based on cycle-consistency either completely discard the information asymmetry between the two domains (a common choice), which leads to inferior translation performance for the asymmetric unpaired image-to-image translation, or have to rely on special task-specific designs and introduce extra loss components. These elaborative designs especially for the relatively harder translation direction from the information-poor domain to the information-rich domain (``poor-to-rich'' translation) require extra labor and are limited to some specific tasks. In this paper, we propose a novel asynchronous generative adversarial network named Async-GAN, which provides a model-agnostic framework for easily turning symmetrical models into powerful asymmetric counterparts that can handle asymmetric unpaired image-to-image translation much better. The key innovation is to iteratively build gradually-improving \textit{intermediate domains} for generating pseudo paired training samples, which provide stronger full supervision for assisting the poor-to-rich translation. Extensive experiments on various asymmetric unpaired translation tasks demonstrate the superiority of the proposal. Furthermore, the proposed training framework could be extended to various Cycle-GAN solutions and achieve a performance gain.
	</p>
      </div>

<div class="section architecture">
	<h2>Architecture</h2>
	<br>
	<center><img src="./Async-GAN/framework.jpg" border="0" width="90%"></center>

    <h2>Results</h2>
	<br>
	<center><img src="./Async-GAN/foggy.jpg" border="0" width="90%"></center>

	<h2>Intermediate domains</h2>
	<br>
	<center><img src="./Async-GAN/intermediate.jpg" border="0" width="90%"></center>

</div>


<div class="section download">

	<h2>Downloads</h2>
<!-- <HR size=2> -->

<ul>
	<li>
		  <a href="" target="_blank">Paper</a>
	</li>


</ul>
</div>


<br>

<div class="section citation">

	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre></pre>
	  </div>
      </div>

</div></div></body></html>