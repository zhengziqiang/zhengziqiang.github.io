<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Not Every Sample is Efficient: Analogical Generative Adversarial Network for Unpaired Image-to-image Translation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Not Every Sample is Efficient: Analogical Generative Adversarial Network for Unpaired Image-to-image Translation">
<meta name="keywords" content="Webpage analysis; Metric learning; Image translation; Information asymmetry">
<meta name="keywords" content="Ziqiang Zheng; 郑自强; Computer Vision;">
<link rel="author" href="https://zhengziqiang.github.io/">

<!-- Fonts and stuff -->
<link href="./css/css" rel="styles<!-- heet" type="text/css">
<link rel="stylesheet" type="text/css" href="./css/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./OST/iconize.css">
<script async="" src="./css/prettify.js.download"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
	<h1>Not Every Sample is Efficient: Analogical Generative Adversarial Network for Unpaired Image-to-image Translation</h1>

	<div class="authors">
        <a href="https://zhengziqiang.github.io/">Ziqiang Zheng</a>
	  <a href="">Jie Yang</a>
        <a href="http://ouc.ai/yuzhibin/index.html/">Zhibin Yu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com.hk/citations?user=vwOQ-UIAAAAJ&hl=zh-CN/">Yubo Wang</a>
	  <a href="http://www.crossocean.world/gywm">Zhijian Sun</a>
	  <a href="http://soi.ouc.edu.cn/archive/info/379">Bing Zheng</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</div>

	<div class="affiliations">
        <p><a href="https://www.ouc.edu.cn/">Ocean University of China</a></p>
	  <p><a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a></p>
	</div class='section teaser'>

	<!-- <div class="venue">Preprint Manuscript (<a href="https://arxiv.org/" target="_blank">Arxiv</a>) 2018</div> -->
      </div>

      <center><img src="./illustration.jpg" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Image translation is to learn an effective mapping function that aims to convert an image from a source domain to another target domain. With the proposal and further developments of generative adversarial networks (GANs), the generative models have achieved great breakthroughs. The image-to-image (I2I) translation methods can mainly fall into two categories: Paired and Unpaired. The former paired methods usually require a large amount of input-output sample pairs to perform one-side image translation, which heavily limits its practicability. To address the lack of the paired samples, CycleGAN and its extensions utilize the cycle-consistency loss to provide an elegant and generic solution to perform the unpaired I2I translation between two domains based on unpaired data. This thread of dual learning-based methods usually adopts the random sampling strategy for optimizing and does not consider the content similarity between samples. However, not every sample is efficient and effective for the desired optimization and leads to optimal convergence. Inspired by analogical learning, which is to utilize the relationships and similarities between sample observations, we propose a novel generic metric-based sampling strategy to effectively select samples from different domains for training. Besides, we introduce a novel analogical adversarial loss to force the model to learn from the effective samples and alleviate the influence of the negative samples. Experimental results on various vision tasks have demonstrated the superior performance of the proposed method. The proposed method is also a generic framework that can be easily extended to other I2I translation methods and result in a performance gain.
	</p>
      </div>

<div class="section architecture">
	<h2>Observation</h2>
	<br>
	<center><img src="./demo.jpg" border="0" width="90%"></center>

    <h2>Architecture</h2>
	<br>
	<center><img src="./network.jpg" border="0" width="90%"></center>

	<h2>Results</h2>
	<br>
	<center><img src="./night2day.jpg" border="0" width="90%"></center>

</div>


<div class="section download">

	<h2>Downloads</h2>
<!-- <HR size=2> -->

<ul>
	<li>
		  <a href="" target="_blank">Paper</a>
	</li>


</ul>
</div>


<br>

<div class="section citation">

	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@article{zheng2022not,
  title={Not every sample is efficient: Analogical generative adversarial network for unpaired image-to-image translation},
  author={Zheng, Ziqiang and Yang, Jie and Yu, Zhibin and Wang, Yubo and Sun, Zhijian and Zheng, Bing},
  journal={Neural Networks},
  volume={148},
  pages={166--175},
  year={2022},
  publisher={Elsevier}
}</pre>
	  </div>
      </div>

</div></div></body></html>